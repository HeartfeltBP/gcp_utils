{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from database_tools.processing.cardiac import estimate_spo2\n",
    "from database_tools.processing.detect import detect_peaks, detect_notches\n",
    "from database_tools.tools.dataset import ConfigMapper\n",
    "from gcp_utils import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_ppg(sig: list, cm: ConfigMapper):\n",
    "    \"\"\"Remove data too far from the signal medium (handles large noise and motion artifacts).\n",
    "       Also, calculate peaks in longest run of good data.\n",
    "    \"\"\"\n",
    "    # Prep data\n",
    "    sig = np.array(sig).reshape(-1)\n",
    "    sig = np.array(sig, dtype=np.float32)\n",
    "    sig[np.isnan(sig)] = 0\n",
    "\n",
    "    # Remove signal outliers\n",
    "    sig, i, j = _remove_sig_outliers(sig, cm.deploy.sig_amp_thresh)\n",
    "\n",
    "    # get, remove outliers from, and order peaks\n",
    "    idx = detect_peaks(sig[i:j])\n",
    "    peaks, troughs = idx['peaks'] + i, idx['troughs'] + i\n",
    "    peaks = _remove_peak_outliers(sig, peaks, cm.deploy.peak_amp_thresh, cm.deploy.peak_dist_thresh)\n",
    "    troughs = _remove_peak_outliers(sig, troughs, cm.deploy.peak_amp_thresh, cm.deploy.peak_dist_thresh)\n",
    "\n",
    "    idx = dict(peaks=peaks, troughs=troughs)\n",
    "    idx = _get_ordered_idx(idx)\n",
    "\n",
    "    peaks, troughs = idx['peaks'], idx['troughs']\n",
    "\n",
    "    peaks_troughs = np.concatenate([peaks, troughs])\n",
    "    min_idx = np.min(peaks_troughs)\n",
    "    max_idx = np.max(peaks_troughs)\n",
    "    sig[0:min_idx-1] = sig[i]\n",
    "    sig[max_idx+1::] = sig[j]\n",
    "    return sig, idx\n",
    "\n",
    "def _get_ordered_idx(idx: dict) -> Tuple[list, list]:\n",
    "    \"\"\"Takes a list of peaks and troughs and removes\n",
    "       out of order elements. Regardless of which occurs first,\n",
    "       a peak or a trough, a peak must be followed by a trough\n",
    "       and vice versa.\n",
    "\n",
    "    Algorithm (if peaks start first)\n",
    "    ---------\n",
    "    - Loop through values starting with first peak\n",
    "    - Is peak before valley?\n",
    "        YES -> Is next peak after valley?\n",
    "            YES -> Append peak and valley. Get next peak and valley.\n",
    "            NO  -> Get next peak.\n",
    "        NO  -> Get next valley.\n",
    "\n",
    "    Args:\n",
    "        peaks (list): Signal peaks.\n",
    "        troughs (list): Signal troughs.\n",
    "\n",
    "    Returns:\n",
    "        first_repaired (list): Input with out of order items removed.\n",
    "        second_repaired (list): Input with out of order items removed.\n",
    "\n",
    "        Items are always returned with peaks idx as first tuple item.\n",
    "    \"\"\"\n",
    "    order_lists = lambda x, y : (x, y, 0, 1) if x[0] < y[0] else (y, x, 1, 0)\n",
    "\n",
    "    # Configure algorithm to start with lowest index.\n",
    "    peaks, troughs = idx['peaks'], idx['troughs']\n",
    "\n",
    "    try:\n",
    "        first, second, flag1, flag2 = order_lists(peaks, troughs)\n",
    "    except IndexError:\n",
    "        return dict(peaks=np.array([]), troughs=np.array([]))\n",
    "\n",
    "    result = dict(first=[], second=[])\n",
    "    i, j = 0, 0\n",
    "    for _ in enumerate(first):\n",
    "        try:\n",
    "            poi_1, poi_2 = first[i], second[j]\n",
    "            if poi_1 < poi_2:  # first point of interest is before second\n",
    "                poi_3 = first[i + 1]\n",
    "                if poi_2 < poi_3:  # second point of interest is before third\n",
    "                    result['first'].append(poi_1)\n",
    "                    result['second'].append(poi_2)\n",
    "                    i += 1; j += 1\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                j += 1\n",
    "        except IndexError: # always thrown in last iteration\n",
    "            result['first'].append(poi_1)\n",
    "            result['second'].append(poi_2)\n",
    "\n",
    "    # remove duplicates and return as peaks, troughs\n",
    "    result['first'] = sorted(list(set(result['first'])))\n",
    "    result['second'] = sorted(list(set(result['second'])))\n",
    "    result = [result['first'], result['second']]\n",
    "    return dict(peaks=np.array(result[flag1]), troughs=np.array(result[flag2]))\n",
    "\n",
    "def _remove_sig_outliers(sig, amp_thresh):\n",
    "    med = np.median(sig)\n",
    "    mask = (sig > (med + amp_thresh)) | (sig < (med - amp_thresh))\n",
    "\n",
    "    temp = sig.copy()\n",
    "    temp[np.where(mask)] = 0\n",
    "    temp[np.where(~mask)] = 1\n",
    "    _, run_starts, run_lengths = _find_runs(temp)\n",
    "\n",
    "    # find the longest run and set all other data to median\n",
    "    k = np.argmax(run_lengths)\n",
    "    print(run_starts)\n",
    "    i, j = run_starts[k], run_starts[k+1]\n",
    "    sig[0:i] = med\n",
    "    sig[j::] = med\n",
    "    return sig, i, j\n",
    "\n",
    "def _remove_peak_outliers(sig, idx, amp_thresh, dist_thresh):\n",
    "    # remove indices whose amplitude is too far from mean\n",
    "    values = sig[idx]\n",
    "    mean = np.mean(values)\n",
    "    mask = np.where( (values < (mean + amp_thresh)) & (values > (mean - amp_thresh)) )\n",
    "    idx = idx[mask]\n",
    "\n",
    "    # remove indices that are too from from each other\n",
    "    diff = np.diff(idx, prepend=idx[0] - 10000, append=idx[-1] + 10000)\n",
    "    delta = int(np.mean(diff[1:-1]))\n",
    "    print(delta)\n",
    "    valid = []\n",
    "    for i, distance1 in enumerate(diff[0:-1]):\n",
    "        distance2 = diff[i+1]\n",
    "        if (distance1 <= (delta + dist_thresh)) & (distance2 <= (delta + dist_thresh)):\n",
    "            valid.append(idx[i])\n",
    "        else:\n",
    "            if len(valid) > 0:\n",
    "                break\n",
    "    return valid\n",
    "\n",
    "def _find_runs(x):\n",
    "    n = x.shape[0]\n",
    "    loc_run_start = np.empty(n, dtype=bool)\n",
    "    loc_run_start[0] = True\n",
    "    np.not_equal(x[:-1], x[1:], out=loc_run_start[1:])\n",
    "    \n",
    "    run_starts = np.nonzero(loc_run_start)[0]\n",
    "    run_values = x[loc_run_start]\n",
    "    run_lengths = np.diff(np.append(run_starts, n))\n",
    "    return (run_values, list(run_starts), run_lengths)\n",
    "\n",
    "def predict_cardiac_metrics(red: list, ir: list, red_idx: list, ir_idx: list, cm: ConfigMapper) -> dict:\n",
    "    red, ir = np.array(red), np.array(ir)\n",
    "    pulse_rate = _calc_pulse_rate(ir_idx, fs=cm.deploy.bpm_fs)\n",
    "    spo2, r = _calc_spo2(red, ir, red_idx, ir_idx)\n",
    "    result = {\n",
    "        'pulse_rate': int(pulse_rate),\n",
    "        'spo2': float(spo2),\n",
    "        'r': float(r)\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def _calc_pulse_rate(idx, fs):\n",
    "    pulse_rate = fs / np.mean(np.diff(idx['peaks'])) * 60\n",
    "    return pulse_rate\n",
    "\n",
    "def _calc_spo2(ppg_red, ppg_ir, red_idx, ir_idx, method='linear'):\n",
    "    red_peaks, red_troughs = red_idx['peaks'], red_idx['troughs']\n",
    "    ir_peaks, ir_troughs = ir_idx['peaks'], ir_idx['troughs']\n",
    "\n",
    "    # choose where to calculate based on shorted list\n",
    "    options = [red_peaks, red_troughs, ir_peaks, ir_troughs]\n",
    "    lengths = [len(x) for x in options]\n",
    "\n",
    "    # Return error code if missing needed value\n",
    "    if 0 in lengths:\n",
    "        return (-1, -1)\n",
    "\n",
    "    i = int(len(options[np.argmin(lengths)]) / 2)\n",
    "\n",
    "    red_high, red_low = np.max(ppg_red[red_peaks[i]]), np.min(ppg_red[red_troughs[i]])\n",
    "    ir_high, ir_low = np.max(ppg_ir[ir_peaks[i]]), np.min(ppg_ir[ir_troughs[i]])\n",
    "\n",
    "    ac_red = red_high - red_low\n",
    "    ac_ir = ir_high - ir_low\n",
    "\n",
    "    r = ( ac_red / red_low ) / ( ac_ir / ir_low )\n",
    "\n",
    "    if method == 'linear':\n",
    "        spo2 = round(104 - (17 * r), 1)\n",
    "    elif method == 'curve':\n",
    "        spo2 = (1.596 * (r ** 2)) + (-34.670 * r) + 112.690\n",
    "    return (spo2, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(red, ir, red_idx, ir_idx):\n",
    "    fig = make_subplots(rows=2, cols=1)\n",
    "    fig.add_scatter(y=red, name='ppg_red', row=1, col=1)\n",
    "    fig.add_scatter(x=red_idx['peaks'], y=red[red_idx['peaks']], mode='markers', row=1, col=1)\n",
    "    fig.add_scatter(x=red_idx['troughs'], y=red[red_idx['troughs']], mode='markers', row=1, col=1)\n",
    "\n",
    "    fig.add_scatter(y=ir, name='ppg_red', row=2, col=1)\n",
    "    fig.add_scatter(x=ir_idx['peaks'], y=ir[ir_idx['peaks']], mode='markers', row=2, col=1)\n",
    "    fig.add_scatter(x=ir_idx['troughs'], y=ir[ir_idx['troughs']], mode='markers', row=2, col=1)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcp_utils.tools.utils import query_collection\n",
    "from firebase_admin import firestore, initialize_app\n",
    "\n",
    "initialize_app()\n",
    "database = firestore.client()\n",
    "col = database.collection(u'bpm_data_test').document('v2iHQmPIVfVW0IuhfZ1yCIegsB52').collection('frames')\n",
    "\n",
    "docs = query_collection(col, 'status', '==', 'new')\n",
    "data = [doc.to_dict() for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfigMapper('/home/cam/Documents/gcp_utils/gcp_utils/config.ini')\n",
    "\n",
    "red_frame, ir_frame = frame['red_frame'], frame['ir_frame']\n",
    "\n",
    "red_clean, red_idx = _preprocess_ppg(red_frame, cm)\n",
    "ir_clean, ir_idx = _preprocess_ppg(ir_frame, cm)\n",
    "\n",
    "result = predict_cardiac_metrics(red_clean, ir_clean, red_idx, ir_idx, cm)\n",
    "\n",
    "plot_data(red_clean, ir_clean, red_idx, ir_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_scatter(y=frame['red_frame'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_high, red_low = 224304, 223715\n",
    "ir_high, ir_low = 244011, 242606\n",
    "\n",
    "ac_red = red_high - red_low\n",
    "ac_ir = ir_high - ir_low\n",
    "\n",
    "r = ( ac_red / red_low ) / ( ac_ir / ir_low )\n",
    "spo2 = round(104 - (17 * r), 1)\n",
    "spo2, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_scatter(y=red_frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp-utils-lIYEkGpv-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae57632980fa8e4c9571bed25e71c397dec37ed8af05aa152e4c0229ee34fd0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
